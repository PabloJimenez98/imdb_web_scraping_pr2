{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d474de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "RMSE en conjunto de prueba: 1.3298\n",
      "Gráfico de análisis supervisado guardado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo.barranco/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con clusters guardado en: ../datasets/cleaned_dataset_with_clusters.csv\n",
      "Gráfico de clusters guardado.\n",
      "Normalidad (Cluster 0): p-value = 0.0016\n",
      "Normalidad (Cluster 1): p-value = 0.0000\n",
      "Homocedasticidad (Levene): p-value = 0.0000\n",
      "Prueba usada: Mann-Whitney U\n",
      "Resultado: estadístico = 97237.0000, p-value = 0.0000\n",
      "Conclusión: Hay evidencia suficiente para rechazar la hipótesis nula.\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   6.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   7.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  13.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   6.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   6.1s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   3.7s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   3.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   6.4s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   3.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   7.6s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   3.7s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   3.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=  14.8s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   4.1s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   3.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.8s\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time=   3.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   3.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  13.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   3.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.8s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   3.7s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=  15.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   7.6s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.7s\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time=   3.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  13.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   6.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=  15.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   6.4s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   3.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  13.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=  15.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   6.4s\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time=   3.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  13.5s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   3.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=7, n_estimators=50; total time=   3.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=  15.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=100; total time=   6.4s\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END ...learning_rate=0.01, max_depth=7, n_estimators=50; total time=   3.7s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=  15.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   7.6s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=  12.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   6.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   7.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  13.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   6.7s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=  12.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   6.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   7.4s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   6.7s\n",
      "[CV] END ....learning_rate=0.2, max_depth=3, n_estimators=50; total time=   3.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   6.6s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=200; total time=  12.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  12.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   6.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  14.0s\n",
      "[CV] END ....learning_rate=0.2, max_depth=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END ....learning_rate=0.2, max_depth=7, n_estimators=50; total time=   3.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=200; total time=  11.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  12.3s\n",
      "[CV] END ....learning_rate=0.1, max_depth=3, n_estimators=50; total time=   3.2s\n",
      "[CV] END ....learning_rate=0.1, max_depth=5, n_estimators=50; total time=   3.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  14.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=200; total time=  11.9s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  12.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   6.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  14.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END ...learning_rate=0.2, max_depth=7, n_estimators=200; total time=  11.9s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import shapiro, levene, ttest_ind, mannwhitneyu\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class MovieAnalysis:\n",
    "    def __init__(self, input_path, output_folder, dataset_output_folder):\n",
    "        self.input_path = input_path\n",
    "        self.output_folder = output_folder\n",
    "        self.dataset_output_folder = dataset_output_folder\n",
    "        self.data = None\n",
    "        self.processed_data = None\n",
    "        self.clusters = None\n",
    "        self._create_folders()\n",
    "\n",
    "    def _create_folders(self):\n",
    "        \"\"\"\n",
    "        Crea los directorios necesarios si no existen.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "        if not os.path.exists(self.dataset_output_folder):\n",
    "            os.makedirs(self.dataset_output_folder)\n",
    "\n",
    "    def load_and_clean_data(self):\n",
    "        \"\"\"\n",
    "        Carga y limpia los datos.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.input_path):\n",
    "            raise FileNotFoundError(f\"Archivo no encontrado en '{self.input_path}'.\")\n",
    "        \n",
    "        data = pd.read_csv(self.input_path)\n",
    "        data['duration'].replace(0, data['duration'].mean(), inplace=True)\n",
    "        data['votes'].fillna(data['votes'].median(), inplace=True)\n",
    "        data['metascore'].fillna(data['metascore'].median(), inplace=True)\n",
    "        data['combined_genres'].fillna(\"Unknown\", inplace=True)\n",
    "        data['directors'].fillna(\"Unknown\", inplace=True)\n",
    "        self.data = data\n",
    "\n",
    "    def supervised_analysis(self):\n",
    "        \"\"\"\n",
    "        Realiza el análisis supervisado para predecir el rating de IMDb.\n",
    "        \"\"\"\n",
    "        X = self.data.drop('imdb_rating', axis=1)\n",
    "        y = self.data['imdb_rating']\n",
    "\n",
    "        # Preprocesamiento\n",
    "        numerical_features = ['duration', 'votes', 'metascore']\n",
    "        categorical_features = ['combined_genres', 'directors']\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Pipeline\n",
    "        gb_model = GradientBoostingRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        }\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=gb_model,\n",
    "            param_grid=param_grid,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', grid_search)\n",
    "        ])\n",
    "\n",
    "        # Dividir los datos\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluación\n",
    "        test_predictions = pipeline.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "        print(f\"RMSE en conjunto de prueba: {rmse:.4f}\")\n",
    "\n",
    "        # Guardar gráfico\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(y_test, test_predictions, alpha=0.7)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel(\"Ratings Reales\")\n",
    "        plt.ylabel(\"Ratings Predichos\")\n",
    "        plt.title(\"Real vs Predicho\")\n",
    "        plt.savefig(f\"{self.output_folder}/real_vs_predicted.png\")\n",
    "        plt.close()\n",
    "        print(\"Gráfico de análisis supervisado guardado.\")\n",
    "\n",
    "    def unsupervised_analysis(self, n_clusters=5):\n",
    "        \"\"\"\n",
    "        Realiza un análisis no supervisado utilizando K-Means y genera clusters.\n",
    "        \"\"\"\n",
    "        numerical_features = ['duration', 'votes', 'metascore']\n",
    "        categorical_features = ['combined_genres', 'directors']\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.processed_data = preprocessor.fit_transform(self.data)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.data['Cluster'] = kmeans.fit_predict(self.processed_data)\n",
    "\n",
    "        # Guardar el dataset con clusters\n",
    "        output_file = os.path.join(self.dataset_output_folder, \"cleaned_dataset_with_clusters.csv\")\n",
    "        self.data.to_csv(output_file, index=False)\n",
    "        print(f\"Dataset con clusters guardado en: {output_file}\")\n",
    "\n",
    "        # Visualización\n",
    "        dense_data = self.processed_data.toarray()\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        reduced_data = pca.fit_transform(dense_data)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=self.data['Cluster'], palette='viridis')\n",
    "        plt.title(\"Clusters de Películas (PCA)\")\n",
    "        plt.xlabel(\"Componente Principal 1\")\n",
    "        plt.ylabel(\"Componente Principal 2\")\n",
    "        plt.legend(title=\"Cluster\")\n",
    "        plt.savefig(f\"{self.output_folder}/clusters_pca.png\")\n",
    "        plt.close()\n",
    "        print(\"Gráfico de clusters guardado.\")\n",
    "\n",
    "    def hypothesis_test(self, cluster1, cluster2):\n",
    "        \"\"\"\n",
    "        Realiza un contraste de hipótesis sobre los votos entre dos clusters.\n",
    "        \"\"\"\n",
    "        votes_cluster1 = self.data[self.data['Cluster'] == cluster1]['votes']\n",
    "        votes_cluster2 = self.data[self.data['Cluster'] == cluster2]['votes']\n",
    "\n",
    "        # Normalidad\n",
    "        p_normal1 = shapiro(votes_cluster1)[1]\n",
    "        p_normal2 = shapiro(votes_cluster2)[1]\n",
    "        print(f\"Normalidad (Cluster {cluster1}): p-value = {p_normal1:.4f}\")\n",
    "        print(f\"Normalidad (Cluster {cluster2}): p-value = {p_normal2:.4f}\")\n",
    "\n",
    "        # Homocedasticidad\n",
    "        p_levene = levene(votes_cluster1, votes_cluster2)[1]\n",
    "        print(f\"Homocedasticidad (Levene): p-value = {p_levene:.4f}\")\n",
    "\n",
    "        # Prueba de hipótesis\n",
    "        if p_normal1 > 0.05 and p_normal2 > 0.05 and p_levene > 0.05:\n",
    "            stat, p_value = ttest_ind(votes_cluster1, votes_cluster2)\n",
    "            test_used = \"t-test\"\n",
    "        else:\n",
    "            stat, p_value = mannwhitneyu(votes_cluster1, votes_cluster2, alternative='two-sided')\n",
    "            test_used = \"Mann-Whitney U\"\n",
    "\n",
    "        print(f\"Prueba usada: {test_used}\")\n",
    "        print(f\"Resultado: estadístico = {stat:.4f}, p-value = {p_value:.4f}\")\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(\"Conclusión: Hay evidencia suficiente para rechazar la hipótesis nula.\")\n",
    "        else:\n",
    "            print(\"Conclusión: No se puede rechazar la hipótesis nula.\")\n",
    "\n",
    "\n",
    "# Flujo Principal\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"../datasets/cleaned_movies_data_new.csv\"  # Ruta al archivo dentro de 'datasets'\n",
    "    output_folder = \"../image\"  # Carpeta para guardar imágenes\n",
    "    dataset_output_folder = \"../datasets\"  # Carpeta para guardar dataset con clusters\n",
    "    analysis = MovieAnalysis(input_path=input_path, output_folder=output_folder, dataset_output_folder=dataset_output_folder)\n",
    "    analysis.load_and_clean_data()\n",
    "    analysis.supervised_analysis()\n",
    "    analysis.unsupervised_analysis(n_clusters=5)\n",
    "    analysis.hypothesis_test(cluster1=0, cluster2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6c300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
